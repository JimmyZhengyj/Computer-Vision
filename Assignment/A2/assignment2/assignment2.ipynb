{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#load images\n",
    "images = {}\n",
    "for alphabet in [\"A\",\"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]:\n",
    "    images[alphabet] = []\n",
    "    for filename in os.listdir(r\"notMNIST_small/\" + alphabet):\n",
    "        img = cv2.imread(\"notMNIST_small/\"+ alphabet + \"/\" + filename, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(alphabet, filename)\n",
    "            continue\n",
    "        img = img.reshape(28*28,)\n",
    "        images[alphabet].append([img])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A .DS_Store\n",
      "F .DS_Store\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# split train, valid and test sets\n",
    "train_set = []\n",
    "valid_set = []\n",
    "test_set = []\n",
    "\n",
    "train_target = []\n",
    "valid_target = []\n",
    "test_target = []\n",
    "\n",
    "target_set = {\"A\": [1,0,0,0,0,0,0,0,0,0],\n",
    "              \"B\": [0,1,0,0,0,0,0,0,0,0],\n",
    "              \"C\": [0,0,1,0,0,0,0,0,0,0],\n",
    "              \"D\": [0,0,0,1,0,0,0,0,0,0],\n",
    "              \"E\": [0,0,0,0,1,0,0,0,0,0],\n",
    "              \"F\": [0,0,0,0,0,1,0,0,0,0],\n",
    "              \"G\": [0,0,0,0,0,0,1,0,0,0],\n",
    "              \"H\": [0,0,0,0,0,0,0,1,0,0],\n",
    "              \"I\": [0,0,0,0,0,0,0,0,1,0],\n",
    "              \"J\": [0,0,0,0,0,0,0,0,0,1]}\n",
    "\n",
    "target_set = {\"A\": [0],\n",
    "              \"B\": [1],\n",
    "              \"C\": [2],\n",
    "              \"D\": [3],\n",
    "              \"E\": [4],\n",
    "              \"F\": [5],\n",
    "              \"G\": [6],\n",
    "              \"H\": [7],\n",
    "              \"I\": [8],\n",
    "              \"J\": [9]}\n",
    "\n",
    "for alphabet in [\"A\",\"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]:\n",
    "    random.shuffle(images[alphabet])\n",
    "    train_set.extend(images[alphabet][:1500])\n",
    "    for i in range(1500): train_target.append(target_set[alphabet])\n",
    "\n",
    "    valid_set.extend(images[alphabet][1500:1600])\n",
    "    for i in range(100): valid_target.append(target_set[alphabet])\n",
    "\n",
    "    test_set.extend(images[alphabet][1600:])\n",
    "    for i in range(len(images[alphabet][1600:])): test_target.append(target_set[alphabet])\n",
    "\n",
    "\n",
    "    \n",
    "train_set = torch.from_numpy(np.array(train_set)).float()\n",
    "valid_set = torch.from_numpy(np.array(valid_set)).float()\n",
    "test_set = torch.from_numpy(np.array(test_set)).float()\n",
    "\n",
    "train_target = torch.from_numpy(np.array(train_target)).long()\n",
    "valid_target = torch.from_numpy(np.array(valid_target)).long()\n",
    "test_target = torch.from_numpy(np.array(test_target)).long()\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# model\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(28*28, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# model\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(28*28, 100500)\n",
    "        self.fc2 = nn.Linear(100500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "learning_rate = 10\n",
    "\n",
    "network = Net2()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Net2' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3d301b784c33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Net2' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "def train(epoch):\n",
    "    learning_rate = 1\n",
    "\n",
    "    network = nn.Sequential(nn.Linear(28*28,1000), nn.ReLU(), nn.Linear(1000,10),nn.Softmax())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    network.train()\n",
    "\n",
    "    # random_order = [i for i in range(len(train_set))]\n",
    "    # random.shuffle(random_order)\n",
    "    # need to shuffle train data\n",
    "    for i in range(epoch):\n",
    "        data = train_set\n",
    "        target = train_target\n",
    "\n",
    "        # print(network[0].weight)\n",
    "        output = network(data)\n",
    "        loss = criterion(output.transpose(1,2), target)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(network[0].weight)\n",
    "        # print(loss.data.item())\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print('Train Loss: {:.6f}'.format(loss.item()))\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "train_pred = network(train_set)\n",
    "print(train_pred.size())\n",
    "\n",
    "print(train_target[0])\n",
    "\n",
    "train_index = [np.argmax(x[0].detach().numpy()) for x in train_pred]\n",
    "acc_num = 0\n",
    "for i in range(len(train_index)):\n",
    "    if train_index[i] == train_target[i][0]:\n",
    "        acc_num += 1\n",
    "print(acc_num) \n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/jimmyzheng/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([15000, 1, 10])\n",
      "tensor([0])\n",
      "1182\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "valid_pred = network(valid_set)\n",
    "print(valid_pred.size())\n",
    "\n",
    "print(valid_target[0])\n",
    "\n",
    "valid_index = [np.argmax(x[0].detach().numpy()) for x in valid_pred]\n",
    "acc_num = 0\n",
    "for i in range(len(valid_index)):\n",
    "    if valid_index[i] == valid_target[i][0]:\n",
    "        acc_num += 1\n",
    "print(acc_num)\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1000, 1, 10])\n",
      "tensor([0])\n",
      "78\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/jimmyzheng/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "a = np.array([2,5,4,6,3])\n",
    "print(np.argmax(a))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "train(30)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Loss: 2.302706\n",
      "Train Loss: 2.302709\n",
      "Train Loss: 2.302703\n",
      "Train Loss: 2.302705\n",
      "Train Loss: 2.302702\n",
      "Train Loss: 2.302701\n",
      "Train Loss: 2.302698\n",
      "Train Loss: 2.302638\n",
      "Train Loss: 2.302635\n",
      "Train Loss: 2.302637\n",
      "Train Loss: 2.302609\n",
      "Train Loss: 2.302571\n",
      "Train Loss: 2.302572\n",
      "Train Loss: 2.302571\n",
      "Train Loss: 2.302568\n",
      "Train Loss: 2.302570\n",
      "Train Loss: 2.302571\n",
      "Train Loss: 2.302570\n",
      "Train Loss: 2.302571\n",
      "Train Loss: 2.302568\n",
      "Train Loss: 2.302570\n",
      "Train Loss: 2.302570\n",
      "Train Loss: 2.302568\n",
      "Train Loss: 2.302570\n",
      "Train Loss: 2.302572\n",
      "Train Loss: 2.302570\n",
      "Train Loss: 2.302571\n",
      "Train Loss: 2.302570\n",
      "Train Loss: 2.302570\n",
      "Train Loss: 2.302570\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "a = torch.Tensor([[1,2,3]])\n",
    "b = torch.Tensor([1]).long()\n",
    "print(a.size())\n",
    "print(b.size())\n",
    "ll = criterion(a,b)\n",
    "print(ll)\n",
    "\n",
    "c = F.softmax(a)\n",
    "print(c)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([1])\n",
      "tensor(1.4076)\n",
      "tensor([[0.0900, 0.2447, 0.6652]])\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/jimmyzheng/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "print(network.fc1.weight)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0080,  0.0253, -0.0162,  ..., -0.0201,  0.0044,  0.0328],\n",
      "        [-0.0244, -0.0154, -0.0177,  ..., -0.0339, -0.0070, -0.0126],\n",
      "        [-0.0033, -0.0236, -0.0048,  ..., -0.0210,  0.0210, -0.0044],\n",
      "        ...,\n",
      "        [-0.0059, -0.0196, -0.0236,  ..., -0.0278,  0.0015,  0.0063],\n",
      "        [ 0.0071,  0.0212, -0.0192,  ..., -0.0005,  0.0301, -0.0225],\n",
      "        [-0.0050, -0.0196,  0.0229,  ..., -0.0270, -0.0003, -0.0024]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "\n",
    "a = []\n",
    "for i in range(10):\n",
    "    a.append(b)\n",
    "a = np.array(a)\n",
    "print(b.shape)\n",
    "print(a.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 2)\n",
      "(10, 2, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "a = np.random.rand(1,3,2)\n",
    "print(a)\n",
    "a = a.transpose((0,2,1))\n",
    "print(a)\n",
    "\n",
    "xx = torch.Tensor([[[0.2,0.3,0.5], [0.4,0.5,0.1]]])\n",
    "print(xx.size())\n",
    "print(xx)\n",
    "\n",
    "xx = xx.transpose(1,2)\n",
    "print(xx)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[0.96775459 0.86893727]\n",
      "  [0.59530076 0.15473133]\n",
      "  [0.70550377 0.19061929]]]\n",
      "[[[0.96775459 0.59530076 0.70550377]\n",
      "  [0.86893727 0.15473133 0.19061929]]]\n",
      "torch.Size([1, 2, 3])\n",
      "tensor([[[0.2000, 0.3000, 0.5000],\n",
      "         [0.4000, 0.5000, 0.1000]]])\n",
      "tensor([[[0.2000, 0.4000],\n",
      "         [0.3000, 0.5000],\n",
      "         [0.5000, 0.1000]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "filenames = \"\"\n",
    "imgs = cv2.imread(\"notMNIST_small/\"+ \"J\" + \"/\" + filenames, cv2.IMREAD_GRAYSCALE)\n",
    "print(imgs.shape)\n",
    "a = []\n",
    "for i in range(1000):\n",
    "    a.append(imgs)\n",
    "print(np.array(a).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(28, 28)\n",
      "(1000, 28, 28)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# model\n",
    "class Models(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Models, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(3, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x)\n",
    "\n",
    "\n",
    "\n",
    "x_data = torch.Tensor([[0.2,0.3,0.5], [0.4,0.5,0.1]])\n",
    "y_data = torch.Tensor([0, 1]).long()\n",
    "model = nn.Sequential(nn.Linear(3,10), nn.ReLU(), nn.Linear(10,2),nn.Softmax())\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1 )\n",
    "\n",
    "for epoch in range(10) :\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion( y_pred, y_data )\n",
    "    print(loss)\n",
    "    print( epoch, loss.data.item() )\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(model[0].weight)\n",
    "print(y_pred.size())\n",
    "print(y_data.size())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.6800, grad_fn=<NllLossBackward>)\n",
      "0 0.6799924373626709\n",
      "Parameter containing:\n",
      "tensor([[-0.1150,  0.3878, -0.2248],\n",
      "        [ 0.0953,  0.2315, -0.0741],\n",
      "        [-0.4820,  0.5382, -0.0987],\n",
      "        [-0.1808, -0.1805,  0.4229],\n",
      "        [ 0.1607, -0.1644, -0.1187],\n",
      "        [-0.5244,  0.5741, -0.0474],\n",
      "        [ 0.1447,  0.4507,  0.3475],\n",
      "        [-0.5194,  0.0140,  0.2324],\n",
      "        [-0.3426, -0.3281,  0.1879],\n",
      "        [ 0.4064, -0.2274, -0.4419]], requires_grad=True)\n",
      "tensor(0.6796, grad_fn=<NllLossBackward>)\n",
      "1 0.6795734167098999\n",
      "Parameter containing:\n",
      "tensor([[-0.1148,  0.3880, -0.2252],\n",
      "        [ 0.0953,  0.2315, -0.0741],\n",
      "        [-0.4820,  0.5382, -0.0987],\n",
      "        [-0.1814, -0.1811,  0.4243],\n",
      "        [ 0.1605, -0.1646, -0.1183],\n",
      "        [-0.5245,  0.5740, -0.0471],\n",
      "        [ 0.1447,  0.4507,  0.3475],\n",
      "        [-0.5194,  0.0140,  0.2324],\n",
      "        [-0.3426, -0.3281,  0.1879],\n",
      "        [ 0.4068, -0.2270, -0.4428]], requires_grad=True)\n",
      "tensor(0.6792, grad_fn=<NllLossBackward>)\n",
      "2 0.6791508197784424\n",
      "Parameter containing:\n",
      "tensor([[-0.1146,  0.3882, -0.2256],\n",
      "        [ 0.0953,  0.2315, -0.0741],\n",
      "        [-0.4820,  0.5382, -0.0987],\n",
      "        [-0.1821, -0.1817,  0.4258],\n",
      "        [ 0.1604, -0.1647, -0.1180],\n",
      "        [-0.5247,  0.5739, -0.0468],\n",
      "        [ 0.1447,  0.4507,  0.3476],\n",
      "        [-0.5194,  0.0140,  0.2324],\n",
      "        [-0.3426, -0.3281,  0.1879],\n",
      "        [ 0.4072, -0.2267, -0.4437]], requires_grad=True)\n",
      "tensor(0.6787, grad_fn=<NllLossBackward>)\n",
      "3 0.6787246465682983\n",
      "Parameter containing:\n",
      "tensor([[-0.1144,  0.3884, -0.2261],\n",
      "        [ 0.0953,  0.2315, -0.0741],\n",
      "        [-0.4820,  0.5382, -0.0987],\n",
      "        [-0.1827, -0.1824,  0.4273],\n",
      "        [ 0.1602, -0.1648, -0.1177],\n",
      "        [-0.5248,  0.5738, -0.0465],\n",
      "        [ 0.1447,  0.4506,  0.3476],\n",
      "        [-0.5194,  0.0140,  0.2324],\n",
      "        [-0.3426, -0.3281,  0.1879],\n",
      "        [ 0.4076, -0.2263, -0.4447]], requires_grad=True)\n",
      "tensor(0.6783, grad_fn=<NllLossBackward>)\n",
      "4 0.6782947778701782\n",
      "Parameter containing:\n",
      "tensor([[-0.1143,  0.3886, -0.2265],\n",
      "        [ 0.0953,  0.2315, -0.0742],\n",
      "        [-0.4820,  0.5382, -0.0987],\n",
      "        [-0.1834, -0.1830,  0.4289],\n",
      "        [ 0.1601, -0.1650, -0.1174],\n",
      "        [-0.5249,  0.5737, -0.0463],\n",
      "        [ 0.1446,  0.4506,  0.3477],\n",
      "        [-0.5194,  0.0140,  0.2324],\n",
      "        [-0.3426, -0.3281,  0.1879],\n",
      "        [ 0.4080, -0.2259, -0.4457]], requires_grad=True)\n",
      "tensor(0.6779, grad_fn=<NllLossBackward>)\n",
      "5 0.6778610348701477\n",
      "Parameter containing:\n",
      "tensor([[-0.1141,  0.3887, -0.2270],\n",
      "        [ 0.0953,  0.2315, -0.0742],\n",
      "        [-0.4820,  0.5382, -0.0987],\n",
      "        [-0.1841, -0.1836,  0.4305],\n",
      "        [ 0.1600, -0.1651, -0.1171],\n",
      "        [-0.5250,  0.5736, -0.0460],\n",
      "        [ 0.1446,  0.4506,  0.3477],\n",
      "        [-0.5194,  0.0140,  0.2324],\n",
      "        [-0.3426, -0.3281,  0.1879],\n",
      "        [ 0.4085, -0.2254, -0.4467]], requires_grad=True)\n",
      "tensor(0.6774, grad_fn=<NllLossBackward>)\n",
      "6 0.677423357963562\n",
      "Parameter containing:\n",
      "tensor([[-0.1138,  0.3889, -0.2275],\n",
      "        [ 0.0954,  0.2315, -0.0743],\n",
      "        [-0.4820,  0.5382, -0.0987],\n",
      "        [-0.1847, -0.1843,  0.4321],\n",
      "        [ 0.1599, -0.1652, -0.1168],\n",
      "        [-0.5251,  0.5734, -0.0457],\n",
      "        [ 0.1446,  0.4506,  0.3478],\n",
      "        [-0.5194,  0.0140,  0.2324],\n",
      "        [-0.3426, -0.3281,  0.1879],\n",
      "        [ 0.4089, -0.2250, -0.4477]], requires_grad=True)\n",
      "tensor(0.6770, grad_fn=<NllLossBackward>)\n",
      "7 0.6769815683364868\n",
      "Parameter containing:\n",
      "tensor([[-0.1136,  0.3891, -0.2280],\n",
      "        [ 0.0954,  0.2315, -0.0743],\n",
      "        [-0.4820,  0.5382, -0.0987],\n",
      "        [-0.1854, -0.1850,  0.4337],\n",
      "        [ 0.1597, -0.1653, -0.1165],\n",
      "        [-0.5253,  0.5733, -0.0454],\n",
      "        [ 0.1446,  0.4505,  0.3479],\n",
      "        [-0.5194,  0.0140,  0.2324],\n",
      "        [-0.3426, -0.3281,  0.1879],\n",
      "        [ 0.4093, -0.2246, -0.4488]], requires_grad=True)\n",
      "tensor(0.6765, grad_fn=<NllLossBackward>)\n",
      "8 0.6765356659889221\n",
      "Parameter containing:\n",
      "tensor([[-0.1134,  0.3894, -0.2285],\n",
      "        [ 0.0954,  0.2316, -0.0744],\n",
      "        [-0.4820,  0.5382, -0.0987],\n",
      "        [-0.1862, -0.1857,  0.4354],\n",
      "        [ 0.1596, -0.1655, -0.1162],\n",
      "        [-0.5254,  0.5732, -0.0451],\n",
      "        [ 0.1445,  0.4505,  0.3479],\n",
      "        [-0.5194,  0.0140,  0.2324],\n",
      "        [-0.3426, -0.3281,  0.1879],\n",
      "        [ 0.4098, -0.2241, -0.4499]], requires_grad=True)\n",
      "tensor(0.6761, grad_fn=<NllLossBackward>)\n",
      "9 0.6760854721069336\n",
      "Parameter containing:\n",
      "tensor([[-0.1132,  0.3896, -0.2290],\n",
      "        [ 0.0954,  0.2316, -0.0744],\n",
      "        [-0.4820,  0.5382, -0.0987],\n",
      "        [-0.1869, -0.1864,  0.4371],\n",
      "        [ 0.1595, -0.1656, -0.1159],\n",
      "        [-0.5255,  0.5731, -0.0448],\n",
      "        [ 0.1445,  0.4505,  0.3480],\n",
      "        [-0.5194,  0.0140,  0.2324],\n",
      "        [-0.3426, -0.3281,  0.1879],\n",
      "        [ 0.4103, -0.2237, -0.4510]], requires_grad=True)\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/jimmyzheng/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print('Number {} : Train Loss: {:.6f}'.format(1,2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number 1 : Train Loss: 2.000000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "a = \"111\"\n",
    "b = \"2222\"\n",
    "\n",
    "print(f\"Epoch {a}, Training loss: {b}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 111, Training loss: 2222\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f3cff9a779be5b794f86c38feaf44c11d9f2b8a8859daea37525ba1a6356b5a6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}